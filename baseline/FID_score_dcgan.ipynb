{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7b08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dc_gan import Generator\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = 'TRUE'\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd70f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#hyper-parameters\n",
    "learning_rate = 2e-4\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd7d646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (generator): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading MNIST weights -- DCGAN\n",
    "\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "gen.load_state_dict(torch.load('mnist_dcgan.pth', map_location=device))\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb1382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=transforms, download=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a7708d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d690e08b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abadagab/anaconda3/envs/S4gan/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/abadagab/anaconda3/envs/S4gan/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: FID score for batch: 63.66679062630564\n",
      "Batch 1: FID score for batch: 64.23622867654387\n",
      "Batch 2: FID score for batch: 65.53893269267701\n",
      "Batch 3: FID score for batch: 62.50555562637081\n",
      "Batch 4: FID score for batch: 56.95146817049405\n",
      "Batch 5: FID score for batch: 68.48163775140586\n",
      "Batch 6: FID score for batch: 60.94788280286966\n",
      "Batch 7: FID score for batch: 61.515903849379896\n",
      "Batch 8: FID score for batch: 58.91108151030264\n",
      "Batch 9: FID score for batch: 65.78818469208659\n",
      "Batch 10: FID score for batch: 67.58763837174195\n",
      "Batch 11: FID score for batch: 62.09312978069812\n",
      "Batch 12: FID score for batch: 57.64450324836312\n",
      "Batch 13: FID score for batch: 70.21582512530071\n",
      "Batch 14: FID score for batch: 67.48408916404006\n",
      "Batch 15: FID score for batch: 63.45694182877597\n",
      "Batch 16: FID score for batch: 60.568962460628654\n",
      "Batch 17: FID score for batch: 68.44977347413376\n",
      "Batch 18: FID score for batch: 65.45670434916946\n",
      "Batch 19: FID score for batch: 61.77498845267752\n",
      "FID Score is equal to:  63.663811132698264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'images, labels = next(iter(test_dataloader))\\nimages = images.repeat(1, 3, 1, 1)\\nreal_images = F.interpolate(images, size = (299, 299))\\nprint(real_images.shape)\\n\\nwith torch.no_grad():\\n    noise = torch.randn(128, NOISE_DIM, 1, 1).to(device)\\n    generated_img = gen(noise)\\n\\ngen_images = F.interpolate(generated_img, size = (299, 299))\\ngen_images = gen_images.repeat(1, 3, 1, 1)\\nmodel = PartialInceptionNetwork()\\nmodel = model.to(device)\\nfid_score = fid_score(real_images, gen_images, BATCH_SIZE, model)\\nprint(\"FID Score:\", fid_score)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_metrics import fid_score, PartialInceptionNetwork\n",
    "\n",
    "#define FID Model\n",
    "model = PartialInceptionNetwork().to(device)\n",
    "\n",
    "#this is only for one particular batch --- Calculation of FID\n",
    "FID_Score = 0\n",
    "i=0\n",
    "for imgs, labels in test_dataloader:\n",
    "    images = imgs.repeat(1, 3, 1, 1)\n",
    "    real_images = F.interpolate(images, size = (299, 299))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(128, NOISE_DIM, 1, 1).to(device)\n",
    "        generated_img = gen(noise)\n",
    "    gen_images = F.interpolate(generated_img, size = (299, 299))\n",
    "    gen_images = gen_images.repeat(1, 3, 1, 1)\n",
    "\n",
    "    fid_score_batch = fid_score(real_images, gen_images, imgs.shape[0], model)\n",
    "    FID_Score += fid_score_batch\n",
    "    print(f\"Batch {i}: FID score for batch: {fid_score_batch}\")\n",
    "    i+=1\n",
    "print(\"FID Score is equal to: \", FID_Score/len(test_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "#If you want to Calculate FID of a single batch in dataloader, you can use this code\n",
    "\"\"\"images, labels = next(iter(test_dataloader))\n",
    "images = images.repeat(1, 3, 1, 1)\n",
    "real_images = F.interpolate(images, size = (299, 299))\n",
    "print(real_images.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(128, NOISE_DIM, 1, 1).to(device)\n",
    "    generated_img = gen(noise)\n",
    "\n",
    "gen_images = F.interpolate(generated_img, size = (299, 299))\n",
    "gen_images = gen_images.repeat(1, 3, 1, 1)\n",
    "model = PartialInceptionNetwork()\n",
    "model = model.to(device)\n",
    "fid_score = fid_score(real_images, gen_images, BATCH_SIZE, model)\n",
    "print(\"FID Score:\", fid_score)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b76e0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (generator): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading CIFAR weights -- DCGAN\n",
    "\n",
    "CHANNELS_IMG = 3\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "gen.load_state_dict(torch.load('cifar_dcgan.pth', map_location=device))\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0b82a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abadagab/anaconda3/envs/S4gan/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/abadagab/anaconda3/envs/S4gan/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: FID score for batch: 195.27387877171245\n",
      "Batch 1: FID score for batch: 189.0330097595008\n",
      "Batch 2: FID score for batch: 191.7647556234041\n",
      "Batch 3: FID score for batch: 192.6418623579033\n",
      "Batch 4: FID score for batch: 188.18977271790635\n",
      "Batch 5: FID score for batch: 191.2678238637945\n",
      "Batch 6: FID score for batch: 192.50876336952322\n",
      "Batch 7: FID score for batch: 194.23309450843254\n",
      "Batch 8: FID score for batch: 187.41242913009876\n",
      "Batch 9: FID score for batch: 189.3634768091946\n",
      "Batch 10: FID score for batch: 189.34463243265714\n",
      "Batch 11: FID score for batch: 191.74598388976386\n",
      "Batch 12: FID score for batch: 193.85380603334295\n",
      "Batch 13: FID score for batch: 193.30794027861083\n",
      "Batch 14: FID score for batch: 193.52679807454547\n",
      "Batch 15: FID score for batch: 200.27921753823068\n",
      "Batch 16: FID score for batch: 189.07756808401243\n",
      "Batch 17: FID score for batch: 189.1383179256813\n",
      "Batch 18: FID score for batch: 196.63813349354632\n",
      "Batch 19: FID score for batch: 197.0750921586427\n",
      "FID Score (CIFAR) is equal to:  192.2838178410252\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "##### CIFAR DATASET ####\n",
    "########################\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='dataset/', train=False, transform=transformations, download=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=True)\n",
    "#define FID Model\n",
    "model = PartialInceptionNetwork().to(device)\n",
    "\n",
    "#this is only for one particular batch --- Calculation of FID\n",
    "FID_Score = 0\n",
    "i=0\n",
    "for imgs, labels in test_dataloader:\n",
    "    #images = imgs.repeat(1, 3, 1, 1)   ----> No need to increase to 3 as CIFAR already has 3 channels unlike MNIST which has 1\n",
    "    real_images = F.interpolate(imgs, size = (299, 299))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(128, NOISE_DIM, 1, 1).to(device)\n",
    "        generated_img = gen(noise)\n",
    "    gen_images = F.interpolate(generated_img, size = (299, 299))\n",
    "    #gen_images = gen_images.repeat(1, 3, 1, 1)\n",
    "\n",
    "    fid_score_batch = fid_score(real_images, gen_images, imgs.shape[0], model)\n",
    "    FID_Score += fid_score_batch\n",
    "    print(f\"Batch {i}: FID score for batch: {fid_score_batch}\")\n",
    "    i+=1\n",
    "print(\"FID Score (CIFAR) is equal to: \", FID_Score/len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010573b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
